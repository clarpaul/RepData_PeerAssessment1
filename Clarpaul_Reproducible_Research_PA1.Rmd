---
title: 'Reproducible Research: Peer Assessment 1'
author: "Paul Clark"
date: "December 25, 2016"
output:
   github_document:
        toc: true
        toc_depth: 4
---

### Introduction

This assignment uses data from a personal activity monitoring device. The data was collected from an individual during October and November, 2012, and includes the number of steps taken in 5 minute intervals. Here, we explore the data to:

 * Understand the distribution of steps per day
 * Profile the number of steps per 5-min period, averaged over the two months
 * Understand the impact of imputing missing values
 * Evaluate the differences between step profiles on weekdays vs. weekends

This document is structured so as to demonstrate fulfillment, in order, of the 9 requirements for a successful submission. (These are listed in the the section titled _**Commit containing full submission**_ within the assignment [_**Review Criteria**_](https://www.coursera.org/learn/reproducible-research/peer/gYyPt/course-project-1).) 

***


#### 1. Load packages and read data

Before starting, we load required packages and read the data.  We also create a directory for permanent storage of the figures that will be created.

```{r message = FALSE, warning = FALSE, collapse=TRUE, cache=FALSE}
# set CRAN mirror for package installation
mir <- "https://cloud.r-project.org"
# Load required packages
if (!require(dplyr)) {install.packages("dplyr", repos = mir); require(dplyr)} # data processing
if (!require(ggplot2)) {install.packages("ggplot2", repos = mir); require(ggplot2)} # plotting
if (!require(zoo)) {install.packages("zoo"); require(zoo)} # imputation of missing values
if (!require(tidyr)) {install.packages("tidyr"); require(tidyr)} # data processing
```
```{r eval=FALSE, echo = FALSE}
# experiment with use of xtable, unsatisfactory to-date
if (!require(xtable)) {install.packages("knitr", repos = mir); require(xtable)}
```
````{r eval = FALSE, echo = FALSE}
# For better knitr formatting defaults using kable [as of 12/28/16, printr doesn't yet work in R 3.3.2]
if (!require(printr)) {install.packages("printr", repos = mir); require(printr)}

```
```{r cache=FALSE}
# Read in the data from the zipped file
conxn <- unz("activity.zip","activity.csv")
# Note: The file consists of 3 columns of data: steps, date, and 5-min interval.
activity <- read.csv(conxn, colClasses = c("integer", "character", "integer"))
# `read.csv` closes the conxn, but we also remove it from our environment.
rm(conxn)
```
```{r}
# Create directory for figures
if (!file.exists("figures")) {dir.create("figures")}
```


***


#### 2. Plot histogram of number of steps taken per day

To obtain the histogram, we first group the `activity` data frame by date, and sum over the steps taken on each date. (Note: In doing so, it is important to set `na.rm = FALSE` in `sum`, as we have found that otherwise, `dplyr::summarize` will include zeroes for dates with `NA`s. We want these dates to be properly identified as containing missing data, so they can be omitted from subsequent calculation.)

```{r cache= FALSE}
# Computing steps by date
stepsbydt_na <- activity %>% group_by(date) %>% summarize(StepsPerDay = sum(steps, na.rm = FALSE))
```

We then plot the desired histogram using `ggplot2::ggplot`, and save it in the figures directory.  We set `na.rm = FALSE`, so that explicit warning messages are generated to confirm the number of days for which the data contains `NA`s.

```{r histNoNA, cache = FALSE, collapse = FALSE}
# Plotting the histogram
(histNoNA <- ggplot(stepsbydt_na, aes(StepsPerDay/1000)) + stat_bin(binwidth = 1, center = 0.5,
        closed = "right", alpha = 0.4, na.rm = FALSE) + 
        scale_x_continuous(minor_breaks = seq(0,22,1), labels = function(x){paste0(x, "K")}) +
        scale_y_continuous(breaks = seq(0,20,2)) +
        labs(x = "Thousands of Steps per Day", y = "Number of Days", title =
        "Histogram of Steps Taken per Day (Excluding NAs)"))
```

***

#### 3. Display mean and median steps per day

We now compute the mean and median steps for the data in the above histogram.  We use the `summary` function for this, as it gives a bit more information than `mean` and `median`.  The median `r median(stepsbydt_na$StepsPerDay, na.rm = TRUE)` and mean `r format(round(mean(stepsbydt_na$StepsPerDay, na.rm = TRUE), 2), nsmall = 2)` are quite close, consistent with a fairly symmetrical distribution.

```{r collapse=FALSE}
print(format(summary(stepsbydt_na$StepsPerDay, digits = 7, na.rm = TRUE), big.mark = ","), quote = FALSE)
```
***


#### 4. Plot time series of average steps per interval

We use `dplyr::group_by` to group by interval, then `dplyr::summarize` to average steps across all days for each interval. Here, we set `na.rm = TRUE` to eliminate `NA` results. (Unless we do this, our `mean` for every interval will be `NA`: as we will see in section (6), for some days, all intervals are `NA`.) To make it easier to check the results vs. experience, we also normalize the `steps` variable to average `StepsPerMin`, dividing by 5.

```{r}
stepsbyint <- activity %>% group_by(interval) %>% summarize(StepsPerMin = mean(steps, na.rm = TRUE)/5)
```

To plot times in 'clock' format, `ggplot2` functions require time variables to be of class `POSIXct`. Therefore, we need to reformat the interval information.  This is not hard, because we have read it in as an integer representing 24-hour clock time, not number of minutes. Note the break between 55 and 100, indicative of clock time (i.e., 55 and 100 represent 0:55 and 1:00).

```{r collapse=TRUE}
stepsbyint$interval[9:15]
```

As preparation for conversion to `POSIXct`, we first change the integers to clock times in *character* format.  For that, we create a reformatting function and vectorize it so that it can be passed to `dplyr::transmute`.

```{r}
# Definition of function to change interval into a character time of day
mktime <- function(tm) {
        digit4_tm <- paste0(paste0(rep("0", 4 - nchar(tm)), collapse = ""), tm, collapse = "")
        digit4_tm <- paste0(substr(digit4_tm, start = 1, stop = 2), ":", substr(digit4_tm, start = 3, stop = 4))
}
# Vectorize fn and use in transmute
vtime <- Vectorize(mktime, SIMPLIFY = TRUE, USE.NAMES = FALSE)
stepsbyintchar <- stepsbyint %>% transmute(time = vtime(interval), StepsPerMin)
```

Now the intervals are expressed as standard 24-hr clock times:

```{r collapse=FALSE, comment= "##"}
format(head(stepsbyintchar), digits = 2)

```


```{r eval = FALSE, include=FALSE, echo = FALSE}
# I am testing this functionality: when coded like this, it gives a huge border around the table
print(xtable(head(stepsbyintchar), digits = 3), type = "html")
```

Next, we convert to `POSIXct` and create the plots (one for display, one for safe-keeping). Note that `POSIXct` times must include a date. However, the date is of no consequence for this plot (so long as we do not use a day with changes to/from Daylight Savings Time).  Therefore we prepare the variable `plottime` from `time` by appending the first date of the period in question, on which we know no conversion to/from DST occurs:

```{r stepTimeSeriesAvg, cache=FALSE, results ='hide'}
stepsbyintchar$plottime <- paste("2012-10-01", stepsbyintchar$time)
# convert character time to actual dttm (POSIXct) value
stepsbyinttm <- stepsbyintchar %>% transmute(plottime = as.POSIXct(plottime, format = "%Y-%m-%d %H:%M"), StepsPerMin)
# plot average number of steps taken over course of day, with time shown in clock hours
(stepTimeSeriesAvg <- ggplot(stepsbyinttm, aes(x = plottime, y = 5*StepsPerMin)) + geom_line() +
        scale_x_datetime(date_labels = "%I %p") + labs(x = "Time of Day",
        y = "Average Steps Per 5-Min Interval", title = "Average steps per 5-min interval throughout day"))
```

***


#### 5. Report the 5-minute interval with maximum average steps

```{r}
(time_maxsteps <- stepsbyintchar[which.max(stepsbyintchar$StepsPerMin), 3:2])
```

The `interval` with maximum average steps is `r format(time_maxsteps$time[[1]], "%I %p")`. During this `interval`, an average of `r round(time_maxsteps$StepsPerMin[[1]], 1)` steps are taken per minute.

***


#### 6. Impute missing data

The total number of missing values in the data set is given by the last table value of the `summary()` function (of course, we could also use `is.na()` to count missing values, but `summary()` gives more information):

```{r collapse = FALSE}
summary(activity$steps, na.rm = FALSE)
```
Note that the number of `NA`s is exactly 8 days of missing data. To show that, we divide the total number of missing values by 24 hours per day  and 12 intervals per hour, equal to `r 12*24` intervals per day:

```{r collapse = TRUE}
num_NAs <- sum(is.na(activity$steps))
(eight <- num_NAs/(12*24))
```

The `r eight` days in question are the following:

```{r collapse = TRUE}
(na_dates <- with(activity, unique(date[is.na(steps)])))
```
##### 6.1 Imputing missing values: Mean Steps by Interval

We now create a new data set `activityimp` by replacing each missing `steps` value in `activity` with the value for the associated 5-minute interval computed by averaging across all days (we call this the __*Mean Steps by Interval*__ method of imputing the data).  Since we know the exact locations of all the `NA`s, we could replace the values rather simply.  Instead, we use a more general procedure by following these steps:  

  1.  Use `is.na()` in the `activity` data frame to subset it on `steps` values which are `NA`, and select from it the vector of `r format(num_NAs, big.mark = ",")` associated `interval` values.  These `r format(num_NAs, big.mark = ",")` `interval` values are not unique, as there are only `r 12*24` intervals per day.
  2.  `match()` the `interval` values from the `activity` data frame to those in `stepsbyint` (the data frame of average `StepsPerMin` by `interval`), 'looking up' the `r format(num_NAs, big.mark = ",")` index values associated with the NAs.
  3.  Use the indices to 'look up' the correct average value of `StepsPerMin` for every interval associated with an `NA`. Using `is.na()` again, put the `StepPerMin` (multiplied by 5) in the right locations in the original data set.
  
```{r collapse=TRUE}
# Create vector of interval values needing imputation
NAintervals <- activity[is.na(activity$steps), "interval"]
# Construct vector containing the corresponding indices in the `stepsbyint` dataframe
NAindices <- match(NAintervals, stepsbyint$interval) # Vector of length = number of NAs
# Use the index values in 'NAindices' to replace the NA values in the orginal data-set
# with those in the mean step per interval dataframe. (Multiply by 5, since we normalized
# the interval data in 'stepsbyint' to a value in average steps per minute).
activityimp <- activity
activityimp$steps[is.na(activity$steps)] <- 5*(stepsbyint$StepsPerMin[NAindices])
```

##### 6.2 Imputing missing values: Interpolation

Next we impute missing days by interpolating values from adjacent days.  We do this by application of a function called `na.approx()` in the package `zoo`.  We start by using `tidyr` to `spread` the `activity` data frame into 'wide' format: a matrix with intervals across the columns, and one date per row. 

```{r message=FALSE}
# Note: we turn the matrix into a tbl_df, for better printing; tbl_df is loaded with one of the
# packages in Hadley's 'tidyverse'
activity_matrix <- spread(data = activity, key = interval, value = steps) %>% tbl_df
```
Note that there is no way to interpolate the first and last dates, as they are at the boundaries of the matrix. (Note that the last date is `r activity_matrix[nrow(activity_matrix), "date"]`.) The other dates are surrounded by non-NA values. 
```{r}
subset(activity_matrix, date %in% na_dates, select = c(1, `830`:`915`))
```
But we can interpolate all the other NA rows.
```{r collapse = FALSE}
activity_matrix_imp <- activity_matrix
activity_matrix_imp[,-1] <- na.approx(object = activity_matrix[,-1], na.rm = FALSE)
def_digits = getOption("digits")
options(digits = 3)
subset(activity_matrix_imp, date %in% na_dates, select = c(1, `830`:`915`))
options(digits = def_digits)
```
To complete the imputation, in the spirit of interpolation (i.e., making imputed rows depend on *local* data) we now set the first and last rows equal to the second and next-to-last rows.
```{r}
activity_matrix_imp2 <- activity_matrix_imp
activity_matrix_imp2[1,-1] <- activity_matrix_imp[2,-1]
activity_matrix_imp2[nrow(activity_matrix),-1] <- activity_matrix[nrow(activity_matrix) - 1,-1]
```
We now plot the results of this method of interpolation on the `r eight` interpolated days of data.
```{r imputation3}
activityimp_int2 <- gather(data = activity_matrix_imp2, key = interval, value = steps, -1, convert = TRUE)
activityimp_int2 <- arrange(activityimp_int2, date) %>% select(interval, date, steps)

activityimp <- transmute(activityimp, interval = interval, date = date, steps = steps, Method = "Mean Steps")
activityimp_int2$Method <- rep("Interpolation", nrow(activityimp_int2))
activityimp_full2 <- rbind(activityimp, activityimp_int2)

activityimp_fullposix2 <- transmute(activityimp_full2, date, plottime =  as.POSIXct(paste("2012-10-01", 
                vtime(interval)), format = "%Y-%m-%d %H:%M"), StepsPerMin = steps/5, Method) 


(ggplot(subset(activityimp_fullposix2, date %in% na_dates), aes(plottime, 5*StepsPerMin, color = Method)) + 
                geom_line() +  facet_wrap(~date) + scale_x_datetime(date_labels = "%H") + 
                labs(x = "Time of Day (24-hr)", y = "Average Steps Per 5-Min Interval", 
                title = "Imputation Method Comparison: Avg steps per 5-min interval"))
```


***


####  7. Evaluate distribution of Steps Per Day with imputed data

Here we utilize the first method of imputing data (the __*Mean Steps by Interval*__ approach). As in section (2), we group our data by `date` and sum over `steps`. 
```{r}
stepsbydtimp <- activityimp %>% group_by(date) %>% summarize(StepsPerDay = sum(steps, na.rm = FALSE))
```

The new `mean` and `median` number of steps taken each day:
```{r}
print(format(summary(stepsbydtimp$StepsPerDay, na.rm = FALSE, digits = 7), big.mark = ",", nsmall =2), quote = FALSE)
```
The difference between `mean` and `median` is now `r round(mean(stepsbydtimp$StepsPerDay) - median(stepsbydtimp$StepsPerDay), 2)`, consistent with a distribution even more symmetrical than before.  We now calculate the difference between imputed and non-imputed data on `mean` and `median`.

```{r collapse=TRUE}
mean(stepsbydtimp$StepsPerDay) - mean(stepsbydt_na$StepsPerDay, na.rm = TRUE)
round(median(stepsbydtimp$StepsPerDay) - median(stepsbydt_na$StepsPerDay, na.rm = TRUE), 2)
```
By adding the imputed values, the mean did not change at all.  This is to be expected, since
all `r eight` days with `NA` values had imputed means equal to the population mean prior to imputing `NA`s.  However, the median *Steps Per Day* moves up slightly, since the original `mean`, `r format(round(mean(stepsbydt_na$StepsPerDay, na.rm = TRUE), 2), 2)`, is slightly higher than the original median, `r median(stepsbydt_na$StepsPerDay, na.rm = TRUE)`.  

We can see the precise change to the distribution most clearly if we do a panel plot, juxtaposing histograms before and after imputing `NA` values. We also examine the histogram for the imputation done by interpolation. To do this, we first compute a factor variable that will be used to differentiate the three histograms in `ggplot`, then combine the data frames (with NAs imputed by *Mean Steps* by interval, with NAs imputed by *Interpolation*, and with *NAs Excluded*), then call `ggplot`.

```{r histPanelNAimputed, results='hide'}
# Computing histogram info for the interpolated imputation
stepsbydtimp_int2 <- activityimp_int2 %>% group_by(date) %>% summarize(StepsPerDay 
                        = sum(steps, na.rm = FALSE))
# Create factor variable to be used in ggplot
NA_Treatment <- as.factor(c(rep("NAs Excluded", nrow(stepsbydt_na)), rep("Mean Steps",
                nrow(stepsbydtimp)), rep("Interpolation", nrow(stepsbydtimp_int2))))
# Combine the three dataframes
stepsbydt_impexc <- rbind(stepsbydt_na, stepsbydtimp, stepsbydtimp_int2)
# Add factor variable to dataframe
stepsbydt_impexc$NA_Treatment <- NA_Treatment
# Make 3-panel plot
(histPanelNAimputed <- ggplot(stepsbydt_impexc, aes(StepsPerDay/1000)) + stat_bin(binwidth = 1,
        center = 0.5, closed = "right", alpha = 0.4, na.rm = FALSE) + 
        scale_x_continuous(minor_breaks = seq(0,22,1), labels = function(x){paste0(x, "K")}) +
        scale_y_continuous(breaks = seq(0,20,4)) +
        facet_grid(NA_Treatment~.) +
        labs(x = "Thousand Steps Per Day", y = "Number of Days", title =
        "Histograms by Treatment of NAs: Number of Days per Thousand Steps"))
```

For the *Mean Steps* imputation, the `r eight` imputed days are placed exactly on top of the mean of the distribution.  For the *Interpolation* method, the re-distribution is more disperse and organic -- perhaps closer to the true distriubtion. Changes in mean and media for *Interpolation* method:

```{r collapse=TRUE}
round(mean(stepsbydtimp_int2$StepsPerDay) - mean(stepsbydt_na$StepsPerDay, na.rm = TRUE), 0)
round(median(stepsbydtimp_int2$StepsPerDay) - median(stepsbydt_na$StepsPerDay, na.rm = TRUE), 0)
```
Both mean and median decrease when the interpolation method is used.

***

#### 8.  Compare average time series of weekday and weekend steps per interval

Creation of this panel plot is done via a process very similar to that in (4) above. However, we take two extra steps to compute separate averages by weekday and weekend day. 

  1. Convert the character `date` field of our data set to `POSIXct` or `Date` format, so that we can call `weekdays()` on it
  2. Categorize the days of the week using `ifelse()`  

We are then in a position to compute the required averages.
```{r}
# As in (4), convert 'date' column of 'activityimp' dataframe to POSIXct format
activityimp$posixdate <- with(activityimp, as.POSIXct(date))
# From 'posixdate' column, create a 'dayofwk' column
activityimp$dayofwk <- with(activityimp, weekdays(posixdate, abbreviate = TRUE))
# From the 'date' column, use fn ifelse() to create colum 'daytype' containing 
# factor variable with two levels: 'weekday' and 'weekend'
activityimp$daytype <- with(activityimp, as.factor(ifelse(dayofwk == "Sat" | dayofwk == "Sun", "weekend","weekday")))
# Use dplyr to simultaneously call 'group_by' on the 'activityimp' dataframe with arguments
# 'interval' and 'daytype', then call 'summarize' on 'steps' using the `mean()` function.
stepsbyintdytype <- activityimp %>% group_by(interval, daytype) %>% summarize(StepsPerMin = mean(steps, na.rm = TRUE)/5)
```

The remaining steps are just like those in (4), with the exception of the call to `facet_grid()` with `ggplot()`.

```{r stepTimeSeriesPanel, results='hide'}
# Leverage previous function 'mktime' to change intervals into character times of day.
# In particular, leverage vectorized form of fn, 'vtime',  for use in dplyr 'transmute'.
stepsbyintchardytype <- stepsbyintdytype %>% as.data.frame %>%
                transmute(interval, time = vtime(interval), StepsPerMin, daytype)
# We prepare the variable 'plottime' by appending the first date of the period in question
stepsbyintchardytype$plottime <- paste("2012-10-01", stepsbyintchardytype$time)
# Note: 'as.date.frame()' needed to remove grouping information prior to transmuting
stepsbytmdytype <- stepsbyintchardytype %>% as.data.frame %>% transmute(plottime =
                as.POSIXct(plottime, format = "%Y-%m-%d %H:%M"), StepsPerMin, daytype)
# Plot average number of steps taken per 5-Min period in panel plot comparing 
# weekday and weekend steps
(stepTimeSeriesPanel <- ggplot(stepsbytmdytype, aes(x = plottime, y = 5*StepsPerMin)) + 
        facet_grid(daytype ~.) + geom_line() +
        scale_x_datetime(date_labels = "%I %p") + labs(x = "Time of Day",
        y = "Average Steps Per 5-Min Interval",
        title = "Average steps per 5-min interval throughout day: Weekdays vs. Weekends"))
```

By eye-balling the charts, one can see that on weekends (as compared to weekdays) our walker
is slower reaching maximum speed, has a lower maximum speed, and is active later in the day. I.e., the distribution of steps has its final local maximum around 8 PM or so on weekends, vs. 7 PM on weekdays.  

***


#### 9. Confirm visibility of all code used

All code used in this analysis is available in this document.  

***

### Appendix: Session Info for Reproducibility of Results

```{r}
sessionInfo()
```
